# 机器阅读理解论文笔记
## [清华nlp组机器阅读理解部分必读论文](https://github.com/thunlp/RCPapers)

### [Leveraging Knowledge Bases in LSTMs for Improving Machine Reading](https://www.aclweb.org/anthology/P17-1132.pdf) 

结合外部知识（KB、KG）来优化LSTM处理entity extraction、event extraction的效果。

#### 模型结构
![model-1](图片\model-1.png)
#### 工作步骤
1. 对所给的知识图谱中的entity 和 relation 进行embedding，使用预训练好的word embedding，并在后续训练中更新
2. 使用BiLSTM处理文本
3. 在每个time step，计算象征来自文本内部信息的向量$S_t$，并从外部KB中选出候选的concepts $V(x_t)$，使用attention机制计算最终结果$m_t$$$m_t=\sum_{i\in V(x_t)}{\alpha _{ti}v_i + \beta _t s_t}$$
4. 将$m_t$与LSTM输出$h_t$相加，得到当前时间步的最终输出$\hat{h_t}$

#### 关键技术
1.对单词和KG中entity和relation的embedding
2.计算两向量的attention时，采用将其中一个向量转置后乘以参数矩阵再乘以另一个向量的形式来计算
$$\alpha(v, h)=(v^TW_\alpha h)$$
#### 数据集
KG部分： WordNet、NELL
文本部分：ACE2005 dataset

### [Key-Value Memory Networks for Directly Reading Documents](https://www.aclweb.org/anthology/D16-1147.pdf)
构建key-value形式的存储结构来完成阅读理解任务。其中，针对可分为纯文本、KB、从文本中IE三类dataset，设计了各自的key-value对构建方式。

#### 模型结构
![model-2](图片\model-2.png)
#### 工作步骤
1. 对数据集建立key-value memory
2. 按一定的启发式规则选出当前问题的候选key-value对
3. 将特征化的question运算为向量形式，并通过迭代，一步一步地将候选key中的信息加入到question中（这一步使用了类似attention的机制），得到最终用于计算结果的question。
4. 将特征化的key运算为向量形式，与上一步得到的question比较相似度，以得到最接近的key，取其value为最终结果。
#### 关键技术
1. 如何构建key-value对
    + 对于sentence，key与value都是sentence本身。
    + 对于KB三元组，key为subject与relation，value为object，同时，对每个三元组的反向三元组，也建立相应的key-value对。
    + 对windows形式的文本（固定长度的一段单词），可建立windows-center对，windows-title对。还有一种看不懂，原文如下：we double the size, D, of the dictionary and encode the center of the window and the value using the second dictionary.
2. 对key和question的表示，文中是使用feature map将问题与Key转换为向量形式的，不知道是人工选取的特征还是训练的embedding
3. 对question的迭代计算。将question和key计算出的相似度作为attention，对value求和，并将结果与这一轮的question相加，并与一个d维方阵相乘，得到新一轮的question，这个过程会重复H次。说实话，不知道这个操作有什么意义。
#### 数据集
构建了**The WikiMovies Benchmark**，使用了三种数据
1. raw Wikipedia documents
2. KG created from the Open Movie Database (OMDb) and MovieLens
3. information extraction performed on the Wikipedia pages to build a KB

[数据集链接](http://fb.ai/babi)

### [Modeling Human Reading with Neural Attention](https://www.aclweb.org/anthology/D16-1009.pdf)
使用hadrd-attention模拟人类在阅读过程中的skip与fixate。
#### 模型结构
![model-3](图片\model-3.png)
#### 工作步骤
1. 训练一个LSTM encoder，对给定的单词序列encode，同时预测下一个词出现的概率。
2. 将encoder的结果和单词序列输入LSTM decoder，预测每个位置单词出现的概率。
3. encode过程中，使用feed-forward network计算当前位置单词的hard attention，若为0则此单词输入到encoder中。
4. 训练一个新的LSTM模型用于计算attention网络的loss。
5. Attention网络与encoder-decoder两部分分开训练，encoder-decoder部分的loss被定义为encodr预测精度、decoder预测精度、hard attention为1的数量之间的运算结果。Attentin部分的loss则定义为计算出的attention与预计分布的计算结果，公式十分复杂。

#### 关键技术
1. 提出人的阅读策略是在平衡尽量大地理解文本和尽量少地付出注意力的前提下取得的。这个思想是其设计此网络的基础。
2. 对于hard-attention的无监督训练，其采用了full-attention，即所有单词都fixate，和随机关注，让attention服从某个二项分布两种模拟方式，用于和模型结果作比较。
3. 采用了网络两部分分开训练的方法，先训练编码解码器，这时采用的注意力网络是二项分布式随机关注，后训练注意力网络，采用了feed-forward network。
4. 挖掘出许多潜在的阅读策略。

#### 数据集
Dundee corpus (Kennedy and Pynte, 2005)
eye-tracking measures computed by Demberg and Keller (2008)

### [BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION](https://arxiv.org/pdf/1611.01603.pdf)
使用问题-文本的双向attention flow进行阅读理解，**是目前被认可的baseline之一**
#### 模型结构
![model-4](图片\model-4.png)
#### 工作步骤
1. 对context与query进行word embedding and character embedding，并把两种embedding结果通过Highway Network进行合并，得到每个time step维数为d的context and query。
2. 将上一步得到的context and query的矩阵表示分别经过BiLSTM模型，保留每个时间步的输出，将两个方向的输出合并，得到每个时间步为长度2d向量的context与query，记为$$H \in R^{2d\times t}$$ $$U\in R^{2d\times j}$$
3. 定义context与query的相似度矩阵$$S_{tj} = \alpha(H_{:t},U_{:j})\in R$$ $$\alpha(h,u) = w^T_{(S)}[h;t;h\circ t]$$其中有$w^T_{(S)}\in R^{6d}$为可训练的参数。
4. 利用相似度矩阵完成context2query attention，即对context的每个时间步，计算其与query每个时间步的attention，并对query的每个时间步进行加权求和，得到$$\tilde U_{:t}=\sum_{j}a_{tj}U_{:j}$$有$\tilde U\in R^{2d\times t}$
5. 利用相似度矩阵完成query2context attention，但与context2attention不同，并不对querry的每个时间步进行atention运算，而是对每个context的时间步取其在相似矩阵中对应行向量的$max_{col}$作为其attention结果，对context进行加权求和，并将得到的列向量复制t次，得到$\tilde H$，整个过程公式如下
$$b = softmax(max_{col}(S))$$ $$\tilde h = \sum_{t}b_tH_{:t}\in R^{2d}$$ $$\tilde H\in R^{2d \times t}$$
6. 将上面得到的$\tilde U$ 和$\tilde H$与context矩阵$H$组合在一起，得到$G$作为下一层的输入。这个过程的公式如下
$$G_{:t} = \beta(H_{:t}, \tilde U_{:t}, \tilde H_{:t})$$ $$\beta(h,\tilde u,\tilde h) = [h;\tilde u;h\circ \tilde u;h\circ \tilde h]\in R^{8d}$$
7. 将得到的$G$经过一个双向LSTM，得到$M$，且有$$M \in R^{2d\times t}$$
8. 将得到的$M$与$G$拼接在一起，与一个长度为$10d$的向量$w_{(p^1)}$的转置相乘，经归一化后得到answer起点的概率分布,$w_{(p^1)}$可训练。
$$p^1=softmax(w^T_{(p^1)}[G;M])$$
9. 将$M$再经过一个双向LSTM网络，得到$M^2$，与$G$拼接在一起后与一个长度为$10d$的向量$w_{(p^2)}$的转置相乘，经归一化后得到answer终点的概率分布，$w_{(p^2)}$可训练.
$$p^2=softmax(w^T_{(p^2)}[G;M^2])$$

#### 关键技术
1. attention的计算方法，本文中计算两个向量attention时是采用让一个参数向量的转置乘以这两个向量的一种拼接，这样的方式来进行的，而不是之前一般采用的点积等方式。$$\alpha(h,u) = w^T_{(S)}[h;t;h\circ t]$$
2. 对attention的使用方法，不同于一般的阅读理解任务中，往往attention机制是用来将各个时间步的结果汇总成一个向量，这篇文章中大量使用了矩阵形式的数据，而不是向量形式，例如，其计算context2query的attention时，对context的每个时间步都进行了计算，使得到的结果还保持为矩阵形式，计算query2context的attention时，其采用了max pooling和tiles使得结果保持为矩阵形式，同时，最后得到answer起点终点的概率分布时，一般的方法都是得到一个 $1\times 2d$ 的向量，再将其与$2d\times t$的参数矩阵相乘，得到在每个时间点上的概率分布，而此文则恰恰与之相反，其是通过大量的拼接和保持，得到size为 $10d\times t$ 的矩阵，之后让一个参数向量转置后与之相乘。
3. 本文对于各种LSTM、attention计算后的结果采取了大量的拼接操作，例如，计算attention时采用了拼接，计算完两种attention后，将结果与之前得到的文本拼接在一起作为下一层的输入，最后计算概率分布时，也是将上一层LSTM的输出和其输入拼接在一起，我认为，这是因为对于机器阅读理解问题，传统的向量表示会导致信息的缺失，拼接而不是丢弃意味着更好的可解释性和更准确的表达。